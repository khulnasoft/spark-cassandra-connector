{"name":"KhulnaSoft Spark Cassandra Connector","tagline":"If you write a Spark application that needs access to Cassandra, this library is for you","body":"# Spark Cassandra Connector\r\n\r\n## Lightning-fast cluster computing with Spark and Cassandra\r\n\r\nThis library lets you expose Cassandra tables as Spark RDDs, write Spark RDDs to Cassandra tables, and\r\nexecute arbitrary CQL queries in your Spark applications.\r\n\r\n## Features\r\n\r\n - Compatible with Apache Cassandra version 2.0 or higher (see table below)\r\n - Compatible with Apache Spark 1.0 through 1.3 (see table below)\r\n - Compatible with Scala 2.10 and 2.11\r\n - Exposes Cassandra tables as Spark RDDs\r\n - Maps table rows to CassandraRow objects or tuples\r\n - Offers customizable object mapper for mapping rows to objects of user-defined classes\r\n - Saves RDDs back to Cassandra by implicit `saveToCassandra` call\r\n - Join with a subset of Cassandra data using `joinWithCassandraTable` call\r\n - Partition RDDs according to Cassandra replication using `repartitionByCassandraReplica` call\r\n - Converts data types between Cassandra and Scala\r\n - Supports all Cassandra data types including collections\r\n - Filters rows on the server side via the CQL `WHERE` clause \r\n - Allows for execution of arbitrary CQL statements\r\n - Plays nice with Cassandra Virtual Nodes\r\n\r\n## Version Compatibility\r\n\r\nThe connector project has several branches, each of which map into different supported versions of \r\nSpark and Cassandra. Refer to the compatibility table below which shows the major.minor \r\nversion range supported between the connector, Spark, Cassandra, and the Cassandra Java driver:\r\n\r\n| Connector | Spark         | Cassandra | Cassandra Java Driver |\r\n| --------- | ------------- | --------- | --------------------- |\r\n| 1.3       | 1.3           | 2.1, 2.0  | 2.1                   |\r\n| 1.2       | 1.2           | 2.1, 2.0  | 2.1                   |\r\n| 1.1       | 1.1, 1.0      | 2.1, 2.0  | 2.1                   |\r\n| 1.0       | 1.0, 0.9      | 2.0       | 2.0                   |\r\n\r\n\r\n## Download\r\nThis project has been published to the Maven Central Repository.\r\nFor SBT to download the connector binaries, sources and javadoc, put this in your project \r\nSBT config:\r\n                                                                                                                           \r\n    libraryDependencies += \"com.khulnasoft.spark\" %% \"spark-cassandra-connector\" % \"1.3.0-M1\"\r\n\r\nIf you want to access the functionality of Connector from Java, you may want to add also a Java API module:\r\n\r\n    libraryDependencies += \"com.khulnasoft.spark\" %% \"spark-cassandra-connector-java\" % \"1.3.0-M1\"\r\n    \r\n## Community\r\n### Reporting Bugs\r\nNew issues should be reported using [JIRA](https://khulnasoft-oss.atlassian.net/browse/SPARKC/).\r\nPlease do not use the built-in GitHub issue tracker.\r\nIt is left for archival purposes and it will be disabled soon.\r\n\r\n### Mailing List\r\nQuestions etc can be submitted to the [user mailing list](http://groups.google.com/a/lists.khulnasoft.com/forum/#!forum/spark-connector-user).\r\n\r\n### Contributing\r\nTo develop this project, we recommend using IntelliJ IDEA. \r\nMake sure you have installed and enabled the Scala Plugin.\r\nOpen the project with IntelliJ IDEA and it will automatically create the project structure\r\nfrom the provided SBT configuration.\r\n\r\nBefore contributing your changes to the project, please make sure that all unit tests and integration tests pass.\r\nDon't forget to add an appropriate entry at the top of CHANGES.txt.\r\nFinally open a pull-request on GitHub and await review. \r\n\r\nIf your pull-request is going to resolve some opened issue, please add *Fixes \\#xx* at the \r\nend of each commit message (where *xx* is the number of the issue).\r\n\r\n## Testing\r\nTo run unit and integration tests:\r\n\r\n    ./sbt/sbt test\r\n    ./sbt/sbt it:test\r\n\r\nBy default, integration tests start up a separate, single Cassandra instance and run Spark in local mode.\r\nIt is possible to run integration tests with your own Cassandra and/or Spark cluster.\r\nFirst, prepare a jar with testing code:\r\n    \r\n    ./sbt/sbt test:package\r\n    \r\nThen copy the generated test jar to your Spark nodes and run:    \r\n\r\n    export IT_TEST_CASSANDRA_HOST=<IP of one of the Cassandra nodes>\r\n    export IT_TEST_SPARK_MASTER=<Spark Master URL>\r\n    ./sbt/sbt it:test\r\n\r\n## License\r\n\r\nCopyright 2014-2015, KhulnaSoft, Inc.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\r\n\r\nhttp://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}
